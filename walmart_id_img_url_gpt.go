package main

import (
	"bufio"
	"compress/gzip"
	"context"
	"crypto/tls"
	"encoding/json"
	"fmt"
	"github.com/PuerkitoBio/goquery"
	"github.com/antchfx/htmlquery"
	"io"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"os"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"

	openai "github.com/sashabaranov/go-openai"
	"github.com/xuri/excelize/v2"
)

var res []Wal

type Wal struct {
	img            []string
	id             string
	desc           string
	rules          string
	organized_desc string
	main_title     string
}

var ids []string
var wg = sync.WaitGroup{}
var wgImg = sync.WaitGroup{}

// cleanText æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤ Unicode è½¬ä¹‰åºåˆ—ã€HTML æ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
func cleanText(s string) string {
	// å»é™¤ Unicode è½¬ä¹‰åºåˆ—
	s = unescapeUnicode(s)

	// å»é™¤ HTML æ ‡ç­¾
	s = removeHTMLTags(s)

	// å»é™¤ç‰¹æ®Šå­—ç¬¦
	s = removeSpecialCharacters(s)

	// å»é™¤æ¢è¡Œç¬¦å’Œ&nbsp;
	s = strings.ReplaceAll(s, "\n", "")
	s = strings.ReplaceAll(s, "&nbsp;", "")
	s = strings.ReplaceAll(s, "&lt;", "")
	s = strings.ReplaceAll(s, "&gt;", "")
	s = strings.ReplaceAll(s, "/strong", "")
	s = strings.ReplaceAll(s, "/n", "")

	return s
}

// unescapeUnicode å°† Unicode è½¬ä¹‰åºåˆ—è½¬æ¢ä¸ºå¯¹åº”çš„å­—ç¬¦
func unescapeUnicode(s string) string {
	re := regexp.MustCompile(`\\u[0-9a-fA-F]{4}`)
	return re.ReplaceAllStringFunc(s, func(match string) string {
		var r rune
		fmt.Sscanf(match, "\\u%04x", &r)
		return string(r)
	})
}

// removeHTMLTags å»é™¤å­—ç¬¦ä¸²ä¸­çš„ HTML æ ‡ç­¾
func removeHTMLTags(s string) string {
	re := regexp.MustCompile(`<[^>]+>`)
	return re.ReplaceAllString(s, "")
}

// removeSpecialCharacters å»é™¤å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦
func removeSpecialCharacters(s string) string {
	// å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦çš„å­—ç¬¦é›†
	specialCharacters := "ğŸ•âœ…"

	// ä½¿ç”¨ strings.Map è¿‡æ»¤æ‰ç‰¹æ®Šå­—ç¬¦
	return strings.Map(func(r rune) rune {
		if strings.ContainsRune(specialCharacters, r) {
			return -1
		}
		return r
	}, s)
}

// downloadImage ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›image.Imageå¯¹è±¡
//func downloadImage(url string) (image.Image, error) {
//	resp, err := http.Get(url)
//	if err != nil {
//		return nil, err
//	}
//	defer resp.Body.Close()
//
//	img, _, err := image.Decode(resp.Body)
//	return img, err
//}

// resizeAndSaveImage ç”¨äºè°ƒæ•´å›¾ç‰‡å¤§å°å¹¶ä¿å­˜
//func resizeAndSaveImage(img image.Image, width, height uint, savePath string) error {
//	newImg := resize.Resize(width, height, img, resize.Lanczos3)
//	out, err := os.Create(savePath)
//	if err != nil {
//		return err
//	}
//	defer out.Close()
//	return jpeg.Encode(out, newImg, nil)
//}

// findJpegUrlsInText ç›´æ¥ä»æ–‡æœ¬ä¸­æå–ä»¥httpså¼€å¤´å¹¶ä»¥.jpegç»“å°¾çš„URL
func findJpegUrlsInText(jsText string) []string {
	// æ­£åˆ™è¡¨è¾¾å¼åŒæ—¶è€ƒè™‘äº†httpsåè®®å’Œ.jpegæ–‡ä»¶æ‰©å±•å
	urlPattern := `https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\.jpeg\b(?:\?[^\s]*)?`
	re := regexp.MustCompile(urlPattern)

	// åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
	urls := re.FindAllString(jsText, -1)
	return urls
}

// normalizeUrl ç”¨äºæ ‡å‡†åŒ–URLï¼Œç§»é™¤æŸ¥è¯¢å‚æ•°ï¼Œåªä¿ç•™åè®®ã€åŸŸåå’Œè·¯å¾„ã€‚
func normalizeUrl(rawUrl string) (string, error) {
	parsedUrl, err := url.Parse(rawUrl)
	if err != nil {
		return "", err
	}

	// é‡æ„URLï¼ŒåªåŒ…å«åè®®ã€ä¸»æœºå’Œè·¯å¾„ï¼Œå¿½ç•¥æŸ¥è¯¢å‚æ•°
	normalizedUrl := fmt.Sprintf("%s://%s%s", parsedUrl.Scheme, parsedUrl.Host, parsedUrl.Path)
	return normalizedUrl, nil
}

func removeDuplicates(urls []string) []string {
	uniqueUrls := make(map[string]bool)
	var result []string

	for _, url := range urls {
		normalizedUrl, err := normalizeUrl(url)
		if err == nil {
			if _, found := uniqueUrls[normalizedUrl]; !found {
				uniqueUrls[normalizedUrl] = true
				result = append(result, url) // å­˜å‚¨åŸå§‹URL
			}
		}
	}
	return result
}

// findJpegUrls æå–ä»¥.jpegç»“å°¾çš„URLï¼Œå¯èƒ½åŒ…å«æŸ¥è¯¢å‚æ•°
// findJpegUrls æå–ä»¥.jpegç»“å°¾çš„URLï¼Œå¯èƒ½åŒ…å«æŸ¥è¯¢å‚æ•°æˆ–ä¸åŒ…å«ä»»ä½•é™„åŠ å‚æ•°
func findJpegUrls(jsTexts []string) []string {
	// æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç›´æ¥ä»¥.jpegç»“å°¾çš„URLï¼Œå¯èƒ½åé¢è·ŸæŸ¥è¯¢å‚æ•°æˆ–ä¸å¸¦ä»»ä½•é¢å¤–å‚æ•°
	urlPattern := `https://i5\.walmartimages\.com/asr/[^\s]+?\.jpeg(\?[^\s]*)?`
	re := regexp.MustCompile(urlPattern)

	var urls []string
	// éå†å­—ç¬¦ä¸²æ•°ç»„ï¼Œå¯¹æ¯ä¸ªå…ƒç´ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
	for _, jsText := range jsTexts {
		matches := re.FindAllString(jsText, -1)
		urls = append(urls, matches...)
	}
	return urls
}

// å®‰å…¨åœ°è®¿é—®æ•°ç»„å…ƒç´ 
func safeAccess(aboutItem []string, indexes ...int) (result string) {
	defer func() {
		if r := recover(); r != nil {
			// å¦‚æœå‘ç”Ÿpanicï¼Œæ•è·å¹¶è®¾ç½®resultä¸ºç©ºå­—ç¬¦ä¸²
			result = ""
		}
	}()

	for _, index := range indexes {
		result += aboutItem[index]
	}

	return result
}

var ch = make(chan int, 6)

func main() {
	log.Println("è‡ªåŠ¨åŒ–è„šæœ¬-walmart_img_gpt")
	log.Println("å¼€å§‹æ‰§è¡Œ...")

	// è¯»å– gpt_order.txt æ–‡ä»¶
	gpt_content, err := os.ReadFile("gpt_order_context.txt")
	if err != nil {
		fmt.Printf("Error reading file: %v\n", err)
		return
	}
	description1 := string(gpt_content)

	// åˆ›å»ºå¥æŸ„
	fi, err := os.Open("id.txt")
	if err != nil {
		panic(err)
	}
	r := bufio.NewReader(fi) // åˆ›å»º Reader

	for {
		lineB, err := r.ReadBytes('\n')
		if len(lineB) > 5 {
			ids = append(ids, strings.TrimSpace(string(lineB)))
		}
		if err != nil {
			break
		}

	}
	log.Println("å…±æœ‰:", len(ids), "ä¸ªid")
	for _, v := range ids {
		ch <- 1
		wg.Add(1)
		go crawler(v, description1)
	}
	wg.Wait()

	xlsx := excelize.NewFile()
	num := 2
	if err := xlsx.SetSheetRow("Sheet1", "A1", &[]interface{}{"id", "å›¾ç‰‡1", "å›¾ç‰‡2", "å›¾ç‰‡3", "å›¾ç‰‡4", "å›¾ç‰‡5", "å›¾ç‰‡6", "å›¾ç‰‡7", "å›¾ç‰‡8", "å›¾ç‰‡9", "å›¾ç‰‡10", "å›¾ç‰‡11", "å›¾ç‰‡12", "å•†å“æè¿°", "éœ€æ±‚", "è¾“å‡ºç»“æœ", "æ ‡é¢˜"}); err != nil {
		log.Println(err)
	}
	//log.Println("res=", res)
	for _, w := range res {
		rowData := []interface{}{w.id}
		for i2 := range w.img {
			rowData = append(rowData, w.img[i2])
		}
		//log.Println("w.img=", w.img)
		//log.Println("imgpath=", imgPath)
		//if _, err := os.Stat(imgPath); err != nil {
		//	log.Printf("File does not exist: %s\n", imgPath)
		//	continue
		//}
		rowData = append(rowData, "")
		// åœ¨è¿™é‡Œæ·»åŠ å›¾ç‰‡ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨
		if err := xlsx.SetSheetRow("Sheet1", "A"+strconv.Itoa(num), &rowData); err != nil {
			log.Println(err)
		}

		fmt.Println("len", len(rowData))
		for len(rowData) < 17 {
			rowData = append(rowData, "")
		}

		rowData[13] = w.desc           // ç¬¬10åˆ—ï¼Œå› ä¸ºåˆ‡ç‰‡ç´¢å¼•ä»0å¼€å§‹
		rowData[14] = w.rules          // ç¬¬11åˆ—
		rowData[15] = w.organized_desc // ç¬¬12åˆ—
		rowData[16] = w.main_title     // ç¬¬13åˆ—

		if err := xlsx.SetSheetRow("Sheet1", "A"+strconv.Itoa(num), &rowData); err != nil {
			log.Println(err)
		}
		num++
	}
	fileName := "out.xlsx"
	for fileNum := 1; exists(fileName); fileNum++ {
		fileName = "out(" + strconv.Itoa(fileNum) + ").xlsx"
	}
	xlsx.SaveAs(fileName)

	log.Println("å…¨éƒ¨å®Œæˆ")

}
func exists(path string) bool {
	_, err := os.Stat(path) //os.Statè·å–æ–‡ä»¶ä¿¡æ¯
	if err != nil {
		if os.IsExist(err) {
			return true
		}
		return false
	}
	return true
}

var letterRunes = []rune("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ")
var IsC = false
var IsC2 = true

func init() {
	rand.Seed(time.Now().UnixNano()) // åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨
}

func generateRandomString(length int) string {
	b := make([]rune, length)
	for i := range b {
		b[i] = letterRunes[rand.Intn(len(letterRunes))]
	}
	return string(b)
}

func newDescriptionByGpt(description string, rules string) (string, error) {
	timeout := time.Minute * 3
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()
	config := openai.DefaultConfig("sk-proj-THEHntGFqKYnt7EQu222T3BlbkFJvOq4uTZCV2tfAcGUbLIO")
	// é…ç½®ä»£ç†
	proxyStr := fmt.Sprintf("http://%s:%d", "127.0.0.1", 51599)
	proxyURL, err := url.Parse(proxyStr)
	if err != nil {
		return "", fmt.Errorf("failed to parse proxy URL: %v", err)
	}
	transport := &http.Transport{
		Proxy: http.ProxyURL(proxyURL),
	}
	config.HTTPClient = &http.Client{
		Transport: transport,
	}
	client := openai.NewClientWithConfig(config)
	resp, err := client.CreateChatCompletion(
		ctx,
		openai.ChatCompletionRequest{
			Model: openai.GPT3Dot5Turbo,
			Messages: []openai.ChatCompletionMessage{
				{
					Role:    openai.ChatMessageRoleUser,
					Content: rules + "\"\"\"" + description + "\"\"\"",
				},
			},
		},
	)

	if err != nil {
		return "", err
	}
	return string(resp.Choices[0].Message.Content), err
}

func crawler(id string, description1 string) {

	//é…ç½®ä»£ç†
	defer func() {
		wg.Done()
		<-ch
	}()
	for i := 0; i < 18; i++ {
		if i != 0 {
			time.Sleep(time.Second * 1)
		}
		proxy_str := fmt.Sprintf("http://%s:%s@%s", "t19932187800946", "wsad123456", "l752.kdltps.com:15818")
		proxy, _ := url.Parse(proxy_str)

		client := &http.Client{Timeout: 10 * time.Second, Transport: &http.Transport{Proxy: http.ProxyURL(proxy), DisableKeepAlives: true, TLSClientConfig: &tls.Config{InsecureSkipVerify: true}}}

		request, _ := http.NewRequest("PUT", "https://www.walmart.com/ip/"+id, nil)

		request.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36")
		request.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7")
		request.Header.Set("Accept-Encoding", "gzip, deflate, br")
		request.Header.Set("Accept-Language", "zh")
		request.Header.Set("Sec-Ch-Ua", `"Not.A/Brand";v="8", "Chromium";v="114", "Google Chrome";v="114"`)
		request.Header.Set("Sec-Ch-Ua-Mobile", "?0")
		request.Header.Set("Sec-Ch-Ua-Platform", `"Windows"`)
		request.Header.Set("Sec-Fetch-Dest", `document`)
		request.Header.Set("Sec-Fetch-Mode", `navigate`)
		request.Header.Set("Sec-Fetch-Site", `none`)
		request.Header.Set("Sec-Fetch-User", `?1`)
		request.Header.Set("Upgrade-Insecure-Requests", `1`)
		var isc = IsC
		if IsC {
			request.Header.Set("Cookie", generateRandomString(10))
		}
		response, err := client.Do(request)

		if err != nil {
			if strings.Contains(err.Error(), "Proxy Bad Serve") || strings.Contains(err.Error(), "context deadline exceeded") {
				log.Println("ä»£ç†IPæ— æ•ˆï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸­")
				log.Println("è¿ç»­å‡ºç°ä»£ç†IPæ— æ•ˆè¯·è”ç³»æˆ‘ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				continue
			} else if strings.Contains(err.Error(), "441") {
				log.Println("ä»£ç†è¶…é¢‘ï¼æš‚åœ10ç§’åç»§ç»­...")
				time.Sleep(time.Second * 10)
				continue
			} else if strings.Contains(err.Error(), "440") {
				log.Println("ä»£ç†å®½å¸¦è¶…é¢‘ï¼æš‚åœ5ç§’åç»§ç»­...")
				time.Sleep(time.Second * 5)
				continue
			} else {
				log.Println("é”™è¯¯ä¿¡æ¯ï¼š" + err.Error())
				log.Println("å‡ºç°é”™è¯¯ï¼Œå¦‚æœåŒidè¿ç»­å‡ºç°è¯·è”ç³»æˆ‘ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				continue
			}
		}
		result := ""
		if response.Header.Get("Content-Encoding") == "gzip" {
			reader, err := gzip.NewReader(response.Body) // gzipè§£å‹ç¼©
			if err != nil {
				log.Println("è§£æbodyé”™è¯¯ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				continue
			}
			defer reader.Close()
			con, err := io.ReadAll(reader)
			if err != nil {
				log.Println("gzipè§£å‹é”™è¯¯ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				continue
			}
			result = string(con)
		} else {
			dataBytes, err := io.ReadAll(response.Body)
			if err != nil {
				if strings.Contains(err.Error(), "Proxy Bad Serve") || strings.Contains(err.Error(), "context deadline exceeded") || strings.Contains(err.Error(), "Service Unavailable") {
					log.Println("ä»£ç†IPæ— æ•ˆï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸­")
					log.Println("è¿ç»­å‡ºç°ä»£ç†IPæ— æ•ˆè¯·è”ç³»æˆ‘ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				} else {
					log.Println("é”™è¯¯ä¿¡æ¯ï¼š" + err.Error())
					log.Println("å‡ºç°é”™è¯¯ï¼Œå¦‚æœåŒidè¿ç»­å‡ºç°è¯·è”ç³»æˆ‘ï¼Œé‡æ–°å¼€å§‹ï¼š" + id)
				}
				continue
			}
			defer response.Body.Close()
			result = string(dataBytes)
		}

		doc, err := goquery.NewDocumentFromReader(strings.NewReader(result))
		if err != nil {
			log.Println("è§£æHTMLé”™è¯¯ï¼š", err)
			return
		}

		//log.Println(result)
		wal := Wal{}
		wal.id = id
		if strings.Contains(result, "This page could not be found.") {
			res = append(res, wal)
			log.Println("id:" + id + "å•†å“ä¸å­˜åœ¨")
			return
		}

		fk := regexp.MustCompile("(Robot or human?)").FindAllStringSubmatch(result, -1)

		if len(fk) > 0 {
			log.Println("id:" + id + " è¢«é£æ§,æ›´æ¢IPç»§ç»­")
			IsC = !isc
			continue
		}
		doc1, err := htmlquery.Parse(strings.NewReader(result))
		if err != nil {
			log.Println("é”™è¯¯ä¿¡æ¯ï¼š" + err.Error())
			return
		}
		firstLink1 := doc.Find("#maincontent > section > main > div.flex.undefined.flex-column.h-100 > div:nth-child(2) > div > div.w_aoqv.w_wRee.w_fdPt > div > div:nth-child(2) > div > div > section > div > div > a").First()
		text1 := firstLink1.Text()

		titleNodes, err := htmlquery.QueryAll(doc1, "//*[@id=\"main-title\"]")
		if err != nil {
			log.Println("æŸ¥è¯¢main-titleé”™è¯¯ï¼š", err)
			return
		}

		// æå– main-title çš„æ–‡æœ¬å†…å®¹
		var titlestrText string
		if len(titleNodes) > 0 {
			titlestrText = htmlquery.InnerText(titleNodes[0])
		}

		// å¦‚æœ titlestrText ä¸­åŒ…å« text1ï¼Œå°† text1 æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
		if strings.Contains(titlestrText, text1) {
			titlestrText = strings.Replace(titlestrText, text1, "", -1)
			//wal.main_title = titlestrText
		}
		wal.main_title = titlestrText
		log.Println("titleStr=", titlestrText)

		var original_text string

		aboutItemReg := regexp.MustCompile(`"longDescription":"([\w\W]*?)","shortDescription":"([\w\W]*?)","`)
		aboutItem := aboutItemReg.FindStringSubmatch(result)
		//if len(aboutItem) > 1 {
		//fmt.Println("origin aboutItem1: ", aboutItem[1])
		//fmt.Println("origin aboutItem2: ", aboutItem[2])
		//}
		// å°è¯•è®¿é—®æ•°ç»„å…ƒç´ 
		original_text = safeAccess(aboutItem, 1, 2)
		original_text = cleanText(original_text)
		fmt.Println(original_text) // è¾“å‡ºç»“æœ
		//log.Println("original_text=", original_text)
		//queryStr := ""
		// åªè·å–ç¬¬ä¸€ä¸ª <a> æ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
		firstLink := doc.Find("#maincontent > section > main > div.flex.undefined.flex-column.h-100 > div:nth-child(2) > div > div.w_aoqv.w_wRee.w_fdPt > div > div:nth-child(2) > div > div > section > div > div > a").First()
		text := firstLink.Text()
		//log.Println("text=", text)
		// å¦‚æœ original_text ä¸­åŒ…å« textï¼Œå°† text æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
		if strings.Contains(original_text, text) {
			original_text = strings.Replace(original_text, text, "", -1)
		}
		log.Println("original_text=", original_text)
		// æŸ¥æ‰¾ shortDescription åŒ¹é…é¡¹
		//shortMatches := shortDescriptionRe.FindStringSubmatch(result)
		//var shortDescription string
		//if len(shortMatches) > 1 {
		//	shortDescription = shortMatches[1]
		//	shortDescription = cleanText(shortDescription)
		//	fmt.Println("Cleaned shortDescription:", shortDescription)
		//} else {
		//	fmt.Println("No shortDescription match found")
		//}
		//original_text = shortDescription + longDescription
		//original_text = shortDescription
		//doc.Find("#maincontent > section > main > div.flex.undefined.flex-column.h-100 > div:nth-child(2) > div > div.w_aoqv.w_wRee.w_fdPt > div > div:nth-child(2) > div > div > section > div > div > a").Each(func(i int, s *goquery.Selection) {
		//	text := s.Text()
		//	if !strings.Contains(original_text, text) {
		//		original_text += text + " "
		//	}
		//})
		//log.Println("original_text=", original_text)

		wal.desc = original_text
		log.Println("shortdescription1=", original_text)
		wal.rules = description1

		reorganized_desc, err := newDescriptionByGpt(original_text, wal.rules)
		if err != nil {
			fmt.Println("gpt generated failed because %v, try again", err)
			reorganized_desc, err = newDescriptionByGpt(original_text, wal.rules)
			if err != nil {
				fmt.Println("gpt generated failed because %v, finish", err)
			}
		}
		keyword := "Product details:"
		index := strings.Index(reorganized_desc, keyword)
		if index != -1 {
			// åœ¨ "Product details:" åé¢æ’å…¥æ¢è¡Œç¬¦
			reorganized_desc = reorganized_desc[:index+len(keyword)] + "\n" + reorganized_desc[index+len(keyword):]
		}
		wal.organized_desc = reorganized_desc
		fmt.Println("reoganized: ", reorganized_desc)

		re := regexp.MustCompile(`"imageInfo":\{"allImages":(\[[\w\W]*?\])`)

		matches := re.FindStringSubmatch(result)

		if len(matches) > 1 {
			listContent := matches[1]

			type Image struct {
				ID       string `json:"id"`
				URL      string `json:"url"`
				Zoomable bool   `json:"zoomable"`
			}
			var images []Image

			err := json.Unmarshal([]byte(listContent), &images)
			if err != nil {
				fmt.Println("Error unmarshalling JSON:", err)
				return
			}

			//var newUrl []string
			for _, image := range images {
				//newUrl = append(newUrl, image.URL)
				wal.img = append(wal.img, image.URL)
			}
			//fmt.Println("New URLs:", newUrl)

		} else {
			fmt.Println("No matches found")
		}
		//img := regexp.MustCompile(`645px"><div style="line-height:0" class="tc b--white ba bw1 b--blue mb2 overflow-hidden br3"><button class="pa0 ma0 bn bg-white b--white pointer" data-testid="item-page-vertical-carousel-hero-image-button"><div class="relative" data-testid="media-thumbnail" style="line-height:0"><img loading="lazy" srcset="([^,^"^?]+)`).FindAllStringSubmatch(result, -1)
		//img1 := regexp.MustCompile(`data-testid="media-thumbnail" style="line-height:0"><img loading="lazy" srcset="([^,^"^?]+)(?:\?[^"]*)?`).FindAllStringSubmatch(result, -1)
		//img := regexp.MustCompile(`data-testid="media-thumbnail" style="line-height:0"><img loading="lazy" srcset="([^,^"^?]+)(?:\?[^"]*)?`).FindAllStringSubmatch(result, -1)
		//imgUrlsReg := regexp.MustCompile(`srcset="([^"]+)"`)
		//imgUrls := imgUrlsReg.FindAllStringSubmatch(result, -1)
		//var largeImageUrls []string
		//for _, img := range imgUrls {
		//	if len(img) > 1 { // ç¡®ä¿æ•è·ç»„å­˜åœ¨
		//		fmt.Println("origin url: ", img[1]) // æ‰“å°å‡ºæ•è·ç»„çš„å†…å®¹ï¼Œå³ src åçš„å€¼
		//		idx := strings.Index(img[1], "?")
		//		var url string
		//		if idx != -1 {
		//			url = img[1][:idx]
		//		}
		//
		//		// fmt.Println("URL before '?':", url)
		//		// urls := strings.Split(srcsetContent, ",")
		//
		//		largeImageUrls = append(largeImageUrls, url+"?odnHeight=2000&odnWidth=2000&odnBg=FFFFFF") //+"?odnHeight=2000&odnWidth=2000&odnBg=FFFFFF"
		//	}
		//}
		//for _, urls4 := range largeImageUrls {
		//	if err != nil {
		//		log.Printf("Failed to download %s: %v\n", urls4, err)
		//		continue
		//	}
		//	wal.img = append(wal.img, urls4)
		//}
		//
		//fmt.Println("concate url:", largeImageUrls)
		//jpegUrls := findJpegUrlsInText(result)
		//uniqueUrls := removeDuplicates(jpegUrls)
		//urls3 := findJpegUrls(uniqueUrls)
		// æ‰“å°ç»“æœå’Œæ·»åŠ åˆ°å›¾ç‰‡åˆ—è¡¨
		//if len(img1) > 0 {
		//	firstMatch := img1[0][1] // è·å–ç¬¬ä¸€ä¸ªåŒ¹é…å€¼
		//	wal.img = append(wal.img, firstMatch)
		//
		//}

		//for _, urls1 := range urls3 {
		//	//img, err := downloadImage(urls1)
		//	if err != nil {
		//		log.Printf("Failed to download %s: %v\n", urls1, err)
		//		continue
		//	}

		//filePath := fmt.Sprintf("img/%s(%d).png", id, x)
		//err = resizeAndSaveImage(img, 1000, 1000, filePath)
		//if err != nil {
		//	log.Printf("Failed to resize or save %s: %v\n", urls1, err)
		//	continue
		//}
		//log.Printf("Downloaded and resized %s to %s\n", urls1, filePath)

		//wal.img = append(wal.img, urls1) // æ·»åŠ åˆ°å›¾ç‰‡åˆ—è¡¨
		//}

		//log.Println(urls)
		//wal.img = append(wal.img, urls...)

		log.Println(id, "å®Œæˆ")
		res = append(res, wal)
		time.Sleep(5)
		return
	}

}
